# -*- coding: utf-8 -*-
"""facial-recog.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_fWmUJ8gEua8uEeaK_pyl76kgY44gLu1
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
#for dirname, _, filenames in os.walk('/kaggle/input'):
    #for filename in filenames:
        #print(os.path.join(dirname, filename))

#libraries for data visualisation
import matplotlib.pyplot as plt
import seaborn as sns

#machine learning libraries
import tensorflow as tf
#library keras
import keras
#preprocess images (2x2 matrix) - convert image to readable nums(matrix- rgb value of each pixcel)
from keras.preprocessing import image
from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import cv2
from tensorflow.keras.applications import VGG16, InceptionResNetV2 #preprocessing images
from keras import regularizers
from tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax #these are math functions

#command to get datasets from kaggle - using inbuilt Kaggle API to download the dataset:

!kaggle datasets download -d ananthu017/emotion-detection-fer

import zipfile
import os

# Ensure the dataset directory exists
os.makedirs('emotion-detection-fer', exist_ok=True)

# Unzip the dataset
with zipfile.ZipFile("emotion-detection-fer.zip","r") as zip_ref:
    zip_ref.extractall("emotion-detection-fer")

train_dir = "/content/emotion-detection-fer/train" #passing the path with training images
test_dir = "/content/emotion-detection-fer/test"

import shutil
import os

# List of directories to delete
directories_to_delete = [
    '/content/emotion-detection-fer/test/disgusted',
    '/content/emotion-detection-fer/test/fearful',
    '/content/emotion-detection-fer/test/surprised',
    '/content/emotion-detection-fer/train/disgusted',
    '/content/emotion-detection-fer/train/fearful',
    '/content/emotion-detection-fer/train/surprised'

    # Add more directories as needed
]

for directory in directories_to_delete:
    try:
        shutil.rmtree(directory)
        print(f"Directory '{directory}' has been deleted successfully.")
    except FileNotFoundError:
        print(f"Directory '{directory}' does not exist.")
    except PermissionError:
        print(f"Permission denied: Unable to delete '{directory}'.")
    except Exception as e:
        print(f"Error deleting '{directory}': {e}")

import zipfile
import os

# List of zip files to unzip
zip_files = ["/content/angry-samples.zip", "/content/happy-samples.zip", "/content/neutral-samples.zip","/content/sad-samples.zip"]



# Unzip each dataset
for zip_file in zip_files:
    with zipfile.ZipFile(zip_file, "r") as zip_ref:
        zip_ref.extractall("sample")

import os
import shutil

# Define the paths for existing and new images
existing_data_dir = '/content/emotion-detection-fer/train'
new_data_dir = '/content/sample'

# List of class names
class_names = ['angry', 'happy', 'sad', 'neutral']

# Function to copy images from new directories to existing class directories
def add_images_to_dataset(existing_dir, new_dir, class_names):
    for class_name in class_names:
        existing_class_dir = os.path.join(existing_dir, class_name)
        new_class_dir = os.path.join(new_dir, f'{class_name}-samples')

        if not os.path.exists(existing_class_dir):
            os.makedirs(existing_class_dir)

        if os.path.exists(new_class_dir):
            for filename in os.listdir(new_class_dir):
                src = os.path.join(new_class_dir, filename)
                dst = os.path.join(existing_class_dir, filename)
                shutil.copy(src, dst)
                print(f"Added {filename} to {existing_class_dir}")
        else:
            print(f"No new images found for class {class_name}")

# Add images to the existing dataset
add_images_to_dataset(existing_data_dir, new_data_dir, class_names)

img_size = 48 #original size of the image

train_datagen = ImageDataGenerator(#rotation_range = 180,
                                         width_shift_range = 0.1,
                                         height_shift_range = 0.1,
                                         horizontal_flip = True,
                                         rescale = 1./255,
                                         #zoom_range = 0.2,
                                         validation_split = 0.2
                                        )
validation_datagen = ImageDataGenerator(rescale = 1./255,
                                         validation_split = 0.2)

#reduce img size
train_generator = train_datagen.flow_from_directory(directory = train_dir,
                                                    target_size = (img_size,img_size),
                                                    batch_size = 64,
                                                    # color_mode = "grayscale",
                                                    class_mode = "categorical",
                                                    subset = "training"
                                                   )
validation_generator = validation_datagen.flow_from_directory( directory = test_dir,
                                                              target_size = (img_size,img_size),
                                                              batch_size = 64,
                                                              # color_mode = "grayscale",
                                                              class_mode = "categorical",
                                                              subset = "validation"
                                                              #validation is testing
                                                             )

"""# First trial model"""

model= tf.keras.models.Sequential()
#convo2d is a type of model
model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48,1)))
model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(128,(5,5), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(256,activation = 'relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))

model.add(Dense(512,activation = 'relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))

model.add(Dense(4, activation='softmax'))

model.compile(
    optimizer = Adam(learning_rate=0.0001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
  )

model.summary()

"""# ResNet Model"""

from tensorflow.keras.applications import ResNet50  #import resnet50
base_model = tf.keras.applications.ResNet50(input_shape=(48,48,3),include_top=False,weights="imagenet") #loading a pretrained resnet50 model
from tensorflow.keras.layers import Dense, Flatten, Dropout,BatchNormalization ,Activation

# Freezing Layers

for layer in base_model.layers[:-4]:
    layer.trainable=False

model=Sequential()
model.add(base_model)
model.add(Dropout(0.5))
model.add(Flatten())
model.add(BatchNormalization())
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dense(4,activation='softmax'))

model.compile(
    optimizer = Adam(learning_rate=0.0001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
  )

model.summary()

from tensorflow.keras.utils import plot_model
from IPython.display import Image
plot_model(model, to_file='/content/drive/MyDrive/Facial Recognition/resnet.png', show_shapes=True,show_layer_names=True)
Image(filename='/content/drive/MyDrive/Facial Recognition/resnet.png')

epochs = 60
batch_size = 64

history = model.fit(x = train_generator,epochs = epochs,validation_data = validation_generator)

# Saving the model

model.save('/content/drive/MyDrive/Facial Recognition/models/resnet.keras')

# To load the model
# model = keras.models.load_model('/content/drive/MyDrive/Facial Recognition/models/resnet.keras')

fig , ax = plt.subplots(1,2)
train_acc = history.history['accuracy']
train_loss = history.history['loss']
fig.set_size_inches(12,4)

ax[0].plot(history.history['accuracy'])
ax[0].plot(history.history['val_accuracy'])
ax[0].set_title('Training Accuracy vs Validation Accuracy')
ax[0].set_ylabel('Accuracy')
ax[0].set_xlabel('Epoch')
ax[0].legend(['Train', 'Validation'], loc='upper left')

ax[1].plot(history.history['loss'])
ax[1].plot(history.history['val_loss'])
ax[1].set_title('Training Loss vs Validation Loss')
ax[1].set_ylabel('Loss')
ax[1].set_xlabel('Epoch')
ax[1].legend(['Train', 'Validation'], loc='upper left')

plt.show()

# model.save('model_optimal.h5')

from keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt

# Load the image, resize to 128x128 pixels, and convert to grayscale
img = image.load_img("/content/emotion-detection-fer/test/happy/im1021.png", target_size=(48, 48), color_mode="rgb")

# Convert the image to a numpy array
img = np.array(img)

# Display the image using Matplotlib
plt.imshow(img)
plt.axis('off')  # Turn off axis labels for better visualization
plt.show()

# Print the shape of the image array
print(img.shape)  # prints (128, 128) that is the shape of our image

from keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt

# Load the image, resize to 128x128 pixels, and convert to grayscale
img = image.load_img("/content/emotion-detection-fer/test/sad/im1095.png", target_size=(48, 48), color_mode="grayscale")

# Convert the image to a numpy array
img = np.array(img)

# Display the image using Matplotlib
plt.imshow(img)
plt.axis('off')  # Turn off axis labels for better visualization
plt.show()

# Print the shape of the image array
print(img.shape)  # prints (48, 48) that is the shape of our image

label_dict = {0:'Angry', 1:'Happy', 2:'Neutral', 3:'Sad'}

img = np.expand_dims(img, axis=0) # makes image shape (1, 48, 48)
img = img.reshape(1, 48, 48, 1)
result = model.predict(img)
result = list(result[0])
print(result)

img_index = result.index(max(result))
print(label_dict[img_index])
plt.show()

label_dict = {0:'Angry',1:'Happy',2:'Neutral',3:'Sad'}

img = np.expand_dims(img,axis = 0) #makes image shape (1,48,48)
img = img.reshape(1,48,48,1)
result = model.predict(img)
result = list(result[0])
print(result)

img_index = result.index(max(result))
print(label_dict[img_index])
plt.show()



"""# Deepface

"""

!pip install deepface

from deepface import DeepFace
import cv2
import matplotlib.pyplot as plt
import json

objs = DeepFace.analyze(
  img_path = "/content/IMG_1075.JPG",
  actions = ['age', 'gender', 'race', 'emotion'],
)

img_path = "/content/IMG_1075.JPG"

img = cv2.imread(img_path)

plt.imshow(img[:, :, : :-1])

demography = DeepFace.analyze(img_path)

type(demography)

demography

type(demography)

demo_emotion = demography[0]["dominant_emotion"]
demo_emotion

